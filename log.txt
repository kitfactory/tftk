dogs-vs-cats tmp
Tensor("stack:0", shape=(150, 150, 3), dtype=uint8)
<Policy "mixed_float16", loss_scale=DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0)>
conv2d_1_0
barchnorm_1_0
relu_1_0
start 2 Tensor("relu_1_0/Identity:0", shape=(None, 75, 75, 64), dtype=float16) 75
get_split_attention_unit Tensor("relu_1_0/Identity:0", shape=(None, 75, 75, 64), dtype=float16)
cnn1_2_1
split_batchnorm1_2_1
split_relu_2_1
-----downsample------------------
avg.pooling_2_1
cnn2_2_1
split_batchnorm2_2_1
shortcut_conv_2_1
shortcut_batchnorm_2_1
get_split_attention_unit Tensor("add/Identity:0", shape=(None, 38, 38, 64), dtype=float16)
cnn1_3_1
split_batchnorm1_3_1
split_relu_3_1
-----downsample------------------
avg.pooling_3_1
cnn2_3_1
split_batchnorm2_3_1
shortcut_conv_3_1
shortcut_batchnorm_3_1
get_split_attention_unit Tensor("add_1/Identity:0", shape=(None, 19, 19, 128), dtype=float16)
cnn1_4_1
split_batchnorm1_4_1
split_relu_4_1
-----downsample------------------
avg.pooling_4_1
cnn2_4_1
split_batchnorm2_4_1
shortcut_conv_4_1
shortcut_batchnorm_4_1
get_split_attention_unit Tensor("add_2/Identity:0", shape=(None, 10, 10, 256), dtype=float16)
cnn1_5_1
split_batchnorm1_5_1
split_relu_5_1
-----downsample------------------
avg.pooling_5_1
cnn2_5_1
split_batchnorm2_5_1
shortcut_conv_5_1
shortcut_batchnorm_5_1
final-batchnorm_6_0
final-relu_6_0
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input (InputLayer)              [(None, 150, 150, 3) 0                                            
__________________________________________________________________________________________________
conv2d_1_0 (Conv2D)             (None, 75, 75, 64)   9472        input[0][0]                      
__________________________________________________________________________________________________
barchnorm_1_0 (BatchNormalizati (None, 75, 75, 64)   256         conv2d_1_0[0][0]                 
__________________________________________________________________________________________________
relu_1_0 (Activation)           (None, 75, 75, 64)   0           barchnorm_1_0[0][0]              
__________________________________________________________________________________________________
cnn1_2_1 (Conv2D)               (None, 75, 75, 64)   4160        relu_1_0[0][0]                   
__________________________________________________________________________________________________
split_batchnorm1_2_1 (BatchNorm (None, 75, 75, 64)   256         cnn1_2_1[0][0]                   
__________________________________________________________________________________________________
split_relu_2_1 (Activation)     (None, 75, 75, 64)   0           split_batchnorm1_2_1[0][0]       
__________________________________________________________________________________________________
avg.pooling_2_1 (AveragePooling (None, 38, 38, 64)   0           split_relu_2_1[0][0]             
__________________________________________________________________________________________________
cnn2_2_1 (Conv2D)               (None, 38, 38, 64)   4160        avg.pooling_2_1[0][0]            
__________________________________________________________________________________________________
shortcut_conv_2_1 (Conv2D)      (None, 38, 38, 64)   4160        relu_1_0[0][0]                   
__________________________________________________________________________________________________
split_batchnorm2_2_1 (BatchNorm (None, 38, 38, 64)   256         cnn2_2_1[0][0]                   
__________________________________________________________________________________________________
shortcut_batchnorm_2_1 (BatchNo (None, 38, 38, 64)   256         shortcut_conv_2_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 38, 38, 64)   0           split_batchnorm2_2_1[0][0]       
                                                                 shortcut_batchnorm_2_1[0][0]     
__________________________________________________________________________________________________
cnn1_3_1 (Conv2D)               (None, 38, 38, 128)  8320        add[0][0]                        
__________________________________________________________________________________________________
split_batchnorm1_3_1 (BatchNorm (None, 38, 38, 128)  512         cnn1_3_1[0][0]                   
__________________________________________________________________________________________________
split_relu_3_1 (Activation)     (None, 38, 38, 128)  0           split_batchnorm1_3_1[0][0]       
__________________________________________________________________________________________________
avg.pooling_3_1 (AveragePooling (None, 19, 19, 128)  0           split_relu_3_1[0][0]             
__________________________________________________________________________________________________
cnn2_3_1 (Conv2D)               (None, 19, 19, 128)  16512       avg.pooling_3_1[0][0]            
__________________________________________________________________________________________________
shortcut_conv_3_1 (Conv2D)      (None, 19, 19, 128)  8320        add[0][0]                        
__________________________________________________________________________________________________
split_batchnorm2_3_1 (BatchNorm (None, 19, 19, 128)  512         cnn2_3_1[0][0]                   
__________________________________________________________________________________________________
shortcut_batchnorm_3_1 (BatchNo (None, 19, 19, 128)  512         shortcut_conv_3_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 19, 19, 128)  0           split_batchnorm2_3_1[0][0]       
                                                                 shortcut_batchnorm_3_1[0][0]     
__________________________________________________________________________________________________
cnn1_4_1 (Conv2D)               (None, 19, 19, 256)  33024       add_1[0][0]                      
__________________________________________________________________________________________________
split_batchnorm1_4_1 (BatchNorm (None, 19, 19, 256)  1024        cnn1_4_1[0][0]                   
__________________________________________________________________________________________________
split_relu_4_1 (Activation)     (None, 19, 19, 256)  0           split_batchnorm1_4_1[0][0]       
__________________________________________________________________________________________________
avg.pooling_4_1 (AveragePooling (None, 10, 10, 256)  0           split_relu_4_1[0][0]             
__________________________________________________________________________________________________
cnn2_4_1 (Conv2D)               (None, 10, 10, 256)  65792       avg.pooling_4_1[0][0]            
__________________________________________________________________________________________________
shortcut_conv_4_1 (Conv2D)      (None, 10, 10, 256)  33024       add_1[0][0]                      
__________________________________________________________________________________________________
split_batchnorm2_4_1 (BatchNorm (None, 10, 10, 256)  1024        cnn2_4_1[0][0]                   
__________________________________________________________________________________________________
shortcut_batchnorm_4_1 (BatchNo (None, 10, 10, 256)  1024        shortcut_conv_4_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 10, 10, 256)  0           split_batchnorm2_4_1[0][0]       
                                                                 shortcut_batchnorm_4_1[0][0]     
__________________________________________________________________________________________________
cnn1_5_1 (Conv2D)               (None, 10, 10, 512)  131584      add_2[0][0]                      
__________________________________________________________________________________________________
split_batchnorm1_5_1 (BatchNorm (None, 10, 10, 512)  2048        cnn1_5_1[0][0]                   
__________________________________________________________________________________________________
split_relu_5_1 (Activation)     (None, 10, 10, 512)  0           split_batchnorm1_5_1[0][0]       
__________________________________________________________________________________________________
avg.pooling_5_1 (AveragePooling (None, 5, 5, 512)    0           split_relu_5_1[0][0]             
__________________________________________________________________________________________________
cnn2_5_1 (Conv2D)               (None, 5, 5, 512)    262656      avg.pooling_5_1[0][0]            
__________________________________________________________________________________________________
shortcut_conv_5_1 (Conv2D)      (None, 5, 5, 512)    131584      add_2[0][0]                      
__________________________________________________________________________________________________
split_batchnorm2_5_1 (BatchNorm (None, 5, 5, 512)    2048        cnn2_5_1[0][0]                   
__________________________________________________________________________________________________
shortcut_batchnorm_5_1 (BatchNo (None, 5, 5, 512)    2048        shortcut_conv_5_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 5, 5, 512)    0           split_batchnorm2_5_1[0][0]       
                                                                 shortcut_batchnorm_5_1[0][0]     
__________________________________________________________________________________________________
final-batchnorm_6_0 (BatchNorma (None, 5, 5, 512)    2048        add_3[0][0]                      
__________________________________________________________________________________________________
final-relu_6_0 (Activation)     (None, 5, 5, 512)    0           final-batchnorm_6_0[0][0]        
__________________________________________________________________________________________________
global_average_pooling2d (Globa (None, 512)          0           final-relu_6_0[0][0]             
==================================================================================================
Total params: 726,592
Trainable params: 719,680
Non-trainable params: 6,912
__________________________________________________________________________________________________
has tmp 0
has tmp 1
has tmp 10
has tmp 11
has tmp 12
has tmp 13
has tmp 14
has tmp 15
has tmp 16
has tmp 17
has tmp 18
has tmp 19
has tmp 2
has tmp 20
has tmp 21
has tmp 22
has tmp 23
has tmp 24
has tmp 25
has tmp 26
has tmp 27
has tmp 28
has tmp 29
has tmp 3
has tmp 30
has tmp 31
has tmp 32
has tmp 33
has tmp 34
has tmp 35
has tmp 36
has tmp 37
has tmp 38
has tmp 39
has tmp 4
has tmp 40
has tmp 41
has tmp 42
has tmp 43
has tmp 44
has tmp 45
has tmp 46
has tmp 47
has tmp 48
has tmp 49
has tmp 5
has tmp 50
has tmp 51
has tmp 52
has tmp 53
has tmp 54
has tmp 55
has tmp 56
has tmp 57
has tmp 6
has tmp 7
has tmp 8
has tmp 9
Callback-TensorBoard
Callback-ModelCheckPoint
Callback-ReduceOnPlateau
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input (InputLayer)              [(None, 150, 150, 3) 0                                            
__________________________________________________________________________________________________
conv2d_1_0 (Conv2D)             (None, 75, 75, 64)   9472        input[0][0]                      
__________________________________________________________________________________________________
barchnorm_1_0 (BatchNormalizati (None, 75, 75, 64)   256         conv2d_1_0[0][0]                 
__________________________________________________________________________________________________
relu_1_0 (Activation)           (None, 75, 75, 64)   0           barchnorm_1_0[0][0]              
__________________________________________________________________________________________________
cnn1_2_1 (Conv2D)               (None, 75, 75, 64)   4160        relu_1_0[0][0]                   
__________________________________________________________________________________________________
split_batchnorm1_2_1 (BatchNorm (None, 75, 75, 64)   256         cnn1_2_1[0][0]                   
__________________________________________________________________________________________________
split_relu_2_1 (Activation)     (None, 75, 75, 64)   0           split_batchnorm1_2_1[0][0]       
__________________________________________________________________________________________________
avg.pooling_2_1 (AveragePooling (None, 38, 38, 64)   0           split_relu_2_1[0][0]             
__________________________________________________________________________________________________
cnn2_2_1 (Conv2D)               (None, 38, 38, 64)   4160        avg.pooling_2_1[0][0]            
__________________________________________________________________________________________________
shortcut_conv_2_1 (Conv2D)      (None, 38, 38, 64)   4160        relu_1_0[0][0]                   
__________________________________________________________________________________________________
split_batchnorm2_2_1 (BatchNorm (None, 38, 38, 64)   256         cnn2_2_1[0][0]                   
__________________________________________________________________________________________________
shortcut_batchnorm_2_1 (BatchNo (None, 38, 38, 64)   256         shortcut_conv_2_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 38, 38, 64)   0           split_batchnorm2_2_1[0][0]       
                                                                 shortcut_batchnorm_2_1[0][0]     
__________________________________________________________________________________________________
cnn1_3_1 (Conv2D)               (None, 38, 38, 128)  8320        add[0][0]                        
__________________________________________________________________________________________________
split_batchnorm1_3_1 (BatchNorm (None, 38, 38, 128)  512         cnn1_3_1[0][0]                   
__________________________________________________________________________________________________
split_relu_3_1 (Activation)     (None, 38, 38, 128)  0           split_batchnorm1_3_1[0][0]       
__________________________________________________________________________________________________
avg.pooling_3_1 (AveragePooling (None, 19, 19, 128)  0           split_relu_3_1[0][0]             
__________________________________________________________________________________________________
cnn2_3_1 (Conv2D)               (None, 19, 19, 128)  16512       avg.pooling_3_1[0][0]            
__________________________________________________________________________________________________
shortcut_conv_3_1 (Conv2D)      (None, 19, 19, 128)  8320        add[0][0]                        
__________________________________________________________________________________________________
split_batchnorm2_3_1 (BatchNorm (None, 19, 19, 128)  512         cnn2_3_1[0][0]                   
__________________________________________________________________________________________________
shortcut_batchnorm_3_1 (BatchNo (None, 19, 19, 128)  512         shortcut_conv_3_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 19, 19, 128)  0           split_batchnorm2_3_1[0][0]       
                                                                 shortcut_batchnorm_3_1[0][0]     
__________________________________________________________________________________________________
cnn1_4_1 (Conv2D)               (None, 19, 19, 256)  33024       add_1[0][0]                      
__________________________________________________________________________________________________
split_batchnorm1_4_1 (BatchNorm (None, 19, 19, 256)  1024        cnn1_4_1[0][0]                   
__________________________________________________________________________________________________
split_relu_4_1 (Activation)     (None, 19, 19, 256)  0           split_batchnorm1_4_1[0][0]       
__________________________________________________________________________________________________
avg.pooling_4_1 (AveragePooling (None, 10, 10, 256)  0           split_relu_4_1[0][0]             
__________________________________________________________________________________________________
cnn2_4_1 (Conv2D)               (None, 10, 10, 256)  65792       avg.pooling_4_1[0][0]            
__________________________________________________________________________________________________
shortcut_conv_4_1 (Conv2D)      (None, 10, 10, 256)  33024       add_1[0][0]                      
__________________________________________________________________________________________________
split_batchnorm2_4_1 (BatchNorm (None, 10, 10, 256)  1024        cnn2_4_1[0][0]                   
__________________________________________________________________________________________________
shortcut_batchnorm_4_1 (BatchNo (None, 10, 10, 256)  1024        shortcut_conv_4_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 10, 10, 256)  0           split_batchnorm2_4_1[0][0]       
                                                                 shortcut_batchnorm_4_1[0][0]     
__________________________________________________________________________________________________
cnn1_5_1 (Conv2D)               (None, 10, 10, 512)  131584      add_2[0][0]                      
__________________________________________________________________________________________________
split_batchnorm1_5_1 (BatchNorm (None, 10, 10, 512)  2048        cnn1_5_1[0][0]                   
__________________________________________________________________________________________________
split_relu_5_1 (Activation)     (None, 10, 10, 512)  0           split_batchnorm1_5_1[0][0]       
__________________________________________________________________________________________________
avg.pooling_5_1 (AveragePooling (None, 5, 5, 512)    0           split_relu_5_1[0][0]             
__________________________________________________________________________________________________
cnn2_5_1 (Conv2D)               (None, 5, 5, 512)    262656      avg.pooling_5_1[0][0]            
__________________________________________________________________________________________________
shortcut_conv_5_1 (Conv2D)      (None, 5, 5, 512)    131584      add_2[0][0]                      
__________________________________________________________________________________________________
split_batchnorm2_5_1 (BatchNorm (None, 5, 5, 512)    2048        cnn2_5_1[0][0]                   
__________________________________________________________________________________________________
shortcut_batchnorm_5_1 (BatchNo (None, 5, 5, 512)    2048        shortcut_conv_5_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 5, 5, 512)    0           split_batchnorm2_5_1[0][0]       
                                                                 shortcut_batchnorm_5_1[0][0]     
__________________________________________________________________________________________________
final-batchnorm_6_0 (BatchNorma (None, 5, 5, 512)    2048        add_3[0][0]                      
__________________________________________________________________________________________________
final-relu_6_0 (Activation)     (None, 5, 5, 512)    0           final-batchnorm_6_0[0][0]        
__________________________________________________________________________________________________
global_average_pooling2d (Globa (None, 512)          0           final-relu_6_0[0][0]             
__________________________________________________________________________________________________
classify-1 (Flatten)            (None, 512)          0           global_average_pooling2d[0][0]   
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          262656      classify-1[0][0]                 
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2)            1026        activation[0][0]                 
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 2)            0           dense_1[0][0]                    
==================================================================================================
Total params: 990,274
Trainable params: 983,362
Non-trainable params: 6,912
__________________________________________________________________________________________________
Train for 1250 steps, validate for 156 steps
Epoch 1/100

   1/1250 [..............................] - ETA: 4:55:43 - loss: 1.3222 - acc: 0.5312
   2/1250 [..............................] - ETA: 2:28:31 - loss: 2.3244 - acc: 0.4531
   4/1250 [..............................] - ETA: 1:14:29 - loss: 2.2132 - acc: 0.3984
   6/1250 [..............................] - ETA: 49:48 - loss: 1.9141 - acc: 0.4583  
   8/1250 [..............................] - ETA: 37:27 - loss: 1.7616 - acc: 0.4844
  10/1250 [..............................] - ETA: 30:02 - loss: 1.6780 - acc: 0.5031
  13/1250 [..............................] - ETA: 23:10 - loss: 1.6092 - acc: 0.4880
  15/1250 [..............................] - ETA: 20:07 - loss: 1.5755 - acc: 0.4896
  18/1250 [..............................] - ETA: 16:48 - loss: 1.5376 - acc: 0.5087
  21/1250 [..............................] - ETA: 14:26 - loss: 1.5001 - acc: 0.5119
  23/1250 [..............................] - ETA: 13:12 - loss: 1.4873 - acc: 0.5149
  26/1250 [..............................] - ETA: 11:43 - loss: 1.4742 - acc: 0.5120
  29/1250 [..............................] - ETA: 10:31 - loss: 1.4688 - acc: 0.4957
  32/1250 [..............................] - ETA: 9:33 - loss: 1.4505 - acc: 0.4990 
  35/1250 [..............................] - ETA: 8:45 - loss: 1.4334 - acc: 0.5045
  38/1250 [..............................] - ETA: 8:05 - loss: 1.4234 - acc: 0.5033
  41/1250 [..............................] - ETA: 7:30 - loss: 1.4145 - acc: 0.5069
  44/1250 [>.............................] - ETA: 7:00 - loss: 1.4114 - acc: 0.5050
  47/1250 [>.............................] - ETA: 6:34 - loss: 1.4045 - acc: 0.5053
  49/1250 [>.............................] - ETA: 6:19 - loss: 1.3969 - acc: 0.5121
  51/1250 [>.............................] - ETA: 6:05 - loss: 1.3900 - acc: 0.5159
  54/1250 [>.............................] - ETA: 5:45 - loss: 1.3880 - acc: 0.5185
  56/1250 [>.............................] - ETA: 5:33 - loss: 1.3834 - acc: 0.5156
  59/1250 [>.............................] - ETA: 5:17 - loss: 1.3834 - acc: 0.5095
  62/1250 [>.............................] - ETA: 5:02 - loss: 1.3778 - acc: 0.5126
  64/1250 [>.............................] - ETA: 4:53 - loss: 1.3744 - acc: 0.5127
  67/1250 [>.............................] - ETA: 4:41 - loss: 1.3682 - acc: 0.5159
  70/1250 [>.............................] - ETA: 4:29 - loss: 1.3626 - acc: 0.5201
  73/1250 [>.............................] - ETA: 4:19 - loss: 1.3602 - acc: 0.5176
  75/1250 [>.............................] - ETA: 4:12 - loss: 1.3580 - acc: 0.5154
  77/1250 [>.............................] - ETA: 4:06 - loss: 1.3545 - acc: 0.5166
  80/1250 [>.............................] - ETA: 3:57 - loss: 1.3514 - acc: 0.5164
  83/1250 [>.............................] - ETA: 3:49 - loss: 1.3474 - acc: 0.5177
  86/1250 [=>............................] - ETA: 3:41 - loss: 1.3435 - acc: 0.5196
  89/1250 [=>............................] - ETA: 3:34 - loss: 1.3400 - acc: 0.5200
  92/1250 [=>............................] - ETA: 3:28 - loss: 1.3364 - acc: 0.5228
  95/1250 [=>............................] - ETA: 3:22 - loss: 1.3333 - acc: 0.5230
  97/1250 [=>............................] - ETA: 3:18 - loss: 1.3332 - acc: 0.5226
 100/1250 [=>............................] - ETA: 3:12 - loss: 1.3297 - acc: 0.5247
 103/1250 [=>............................] - ETA: 3:07 - loss: 1.3281 - acc: 0.5243
 105/1250 [=>............................] - ETA: 3:03 - loss: 1.3267 - acc: 0.5226
 108/1250 [=>............................] - ETA: 2:59 - loss: 1.3220 - acc: 0.5231
 111/1250 [=>............................] - ETA: 2:54 - loss: 1.3164 - acc: 0.5276
 114/1250 [=>............................] - ETA: 2:50 - loss: 1.3177 - acc: 0.5258
 117/1250 [=>............................] - ETA: 2:46 - loss: 1.3143 - acc: 0.5272
 120/1250 [=>............................] - ETA: 2:42 - loss: 1.3122 - acc: 0.5289
 122/1250 [=>............................] - ETA: 2:39 - loss: 1.3110 - acc: 0.5284
 124/1250 [=>............................] - ETA: 2:37 - loss: 1.3097 - acc: 0.5295
 127/1250 [==>...........................] - ETA: 2:33 - loss: 1.3064 - acc: 0.5308
 130/1250 [==>...........................] - ETA: 2:30 - loss: 1.3036 - acc: 0.5312
 133/1250 [==>...........................] - ETA: 2:27 - loss: 1.3013 - acc: 0.5315
 136/1250 [==>...........................] - ETA: 2:24 - loss: 1.3000 - acc: 0.5276
 139/1250 [==>...........................] - ETA: 2:21 - loss: 1.2978 - acc: 0.5272
 142/1250 [==>...........................] - ETA: 2:18 - loss: 1.2963 - acc: 0.5268
 144/1250 [==>...........................] - ETA: 2:16 - loss: 1.2949 - acc: 0.5265
 147/1250 [==>...........................] - ETA: 2:14 - loss: 1.2921 - acc: 0.5278
 150/1250 [==>...........................] - ETA: 2:11 - loss: 1.2905 - acc: 0.5290
 153/1250 [==>...........................] - ETA: 2:09 - loss: 1.2889 - acc: 0.5272
 156/1250 [==>...........................] - ETA: 2:06 - loss: 1.2878 - acc: 0.5262
 159/1250 [==>...........................] - ETA: 2:04 - loss: 1.2865 - acc: 0.5254
 162/1250 [==>...........................] - ETA: 2:02 - loss: 1.2839 - acc: 0.5260
 164/1250 [==>...........................] - ETA: 2:01 - loss: 1.2814 - acc: 0.5269
 167/1250 [===>..........................] - ETA: 1:59 - loss: 1.2775 - acc: 0.5290
 170/1250 [===>..........................] - ETA: 1:57 - loss: 1.2760 - acc: 0.5289
 173/1250 [===>..........................] - ETA: 1:55 - loss: 1.2737 - acc: 0.5294
 176/1250 [===>..........................] - ETA: 1:53 - loss: 1.2710 - acc: 0.5307
 179/1250 [===>..........................] - ETA: 1:51 - loss: 1.2690 - acc: 0.5311
 182/1250 [===>..........................] - ETA: 1:49 - loss: 1.2670 - acc: 0.5309
 185/1250 [===>..........................] - ETA: 1:48 - loss: 1.2661 - acc: 0.5302
 188/1250 [===>..........................] - ETA: 1:46 - loss: 1.2640 - acc: 0.5301
 191/1250 [===>..........................] - ETA: 1:44 - loss: 1.2628 - acc: 0.5283
 194/1250 [===>..........................] - ETA: 1:43 - loss: 1.2606 - acc: 0.5293
 197/1250 [===>..........................] - ETA: 1:41 - loss: 1.2580 - acc: 0.5314
 200/1250 [===>..........................] - ETA: 1:40 - loss: 1.2563 - acc: 0.5323
 203/1250 [===>..........................] - ETA: 1:39 - loss: 1.2546 - acc: 0.5325
 206/1250 [===>..........................] - ETA: 1:37 - loss: 1.2540 - acc: 0.5306
 209/1250 [====>.........................] - ETA: 1:36 - loss: 1.2519 - acc: 0.5317
 212/1250 [====>.........................] - ETA: 1:35 - loss: 1.2500 - acc: 0.5317
 215/1250 [====>.........................] - ETA: 1:33 - loss: 1.2484 - acc: 0.5315
 218/1250 [====>.........................] - ETA: 1:32 - loss: 1.2468 - acc: 0.5302
 220/1250 [====>.........................] - ETA: 1:31 - loss: 1.2458 - acc: 0.5304
 223/1250 [====>.........................] - ETA: 1:30 - loss: 1.2443 - acc: 0.5296
 226/1250 [====>.........................] - ETA: 1:29 - loss: 1.2425 - acc: 0.5295
 228/1250 [====>.........................] - ETA: 1:28 - loss: 1.2412 - acc: 0.5299
 231/1250 [====>.........................] - ETA: 1:27 - loss: 1.2393 - acc: 0.5303
 233/1250 [====>.........................] - ETA: 1:27 - loss: 1.2385 - acc: 0.5306
 236/1250 [====>.........................] - ETA: 1:25 - loss: 1.2359 - acc: 0.5320
 239/1250 [====>.........................] - ETA: 1:24 - loss: 1.2344 - acc: 0.5323
 242/1250 [====>.........................] - ETA: 1:23 - loss: 1.2334 - acc: 0.5325
 245/1250 [====>.........................] - ETA: 1:22 - loss: 1.2318 - acc: 0.5319
 248/1250 [====>.........................] - ETA: 1:21 - loss: 1.2303 - acc: 0.5325
 251/1250 [=====>........................] - ETA: 1:21 - loss: 1.2288 - acc: 0.5327
 254/1250 [=====>........................] - ETA: 1:20 - loss: 1.2267 - acc: 0.5333
 257/1250 [=====>........................] - ETA: 1:19 - loss: 1.2245 - acc: 0.5347
 260/1250 [=====>........................] - ETA: 1:18 - loss: 1.2238 - acc: 0.5333
 263/1250 [=====>........................] - ETA: 1:17 - loss: 1.2220 - acc: 0.5337
 266/1250 [=====>........................] - ETA: 1:16 - loss: 1.2204 - acc: 0.5329
 269/1250 [=====>........................] - ETA: 1:15 - loss: 1.2182 - acc: 0.5340
 271/1250 [=====>........................] - ETA: 1:15 - loss: 1.2170 - acc: 0.5349
 274/1250 [=====>........................] - ETA: 1:14 - loss: 1.2156 - acc: 0.5348
 277/1250 [=====>........................] - ETA: 1:13 - loss: 1.2139 - acc: 0.5350
 280/1250 [=====>........................] - ETA: 1:12 - loss: 1.2121 - acc: 0.5344
 283/1250 [=====>........................] - ETA: 1:12 - loss: 1.2103 - acc: 0.5349
 286/1250 [=====>........................] - ETA: 1:11 - loss: 1.2084 - acc: 0.5349
 289/1250 [=====>........................] - ETA: 1:10 - loss: 1.2071 - acc: 0.5343
 291/1250 [=====>........................] - ETA: 1:10 - loss: 1.2057 - acc: 0.5348
 294/1250 [======>.......................] - ETA: 1:09 - loss: 1.2041 - acc: 0.5345
 297/1250 [======>.......................] - ETA: 1:08 - loss: 1.2024 - acc: 0.5348
 300/1250 [======>.......................] - ETA: 1:08 - loss: 1.1996 - acc: 0.5370
 303/1250 [======>.......................] - ETA: 1:07 - loss: 1.1979 - acc: 0.5379
 306/1250 [======>.......................] - ETA: 1:06 - loss: 1.1967 - acc: 0.5379
 309/1250 [======>.......................] - ETA: 1:06 - loss: 1.1946 - acc: 0.5391
 311/1250 [======>.......................] - ETA: 1:05 - loss: 1.1934 - acc: 0.5396
 314/1250 [======>.......................] - ETA: 1:05 - loss: 1.1919 - acc: 0.5401
 317/1250 [======>.......................] - ETA: 1:04 - loss: 1.1906 - acc: 0.5392
 320/1250 [======>.......................] - ETA: 1:03 - loss: 1.1890 - acc: 0.5392
 323/1250 [======>.......................] - ETA: 1:03 - loss: 1.1869 - acc: 0.5403
 326/1250 [======>.......................] - ETA: 1:02 - loss: 1.1850 - acc: 0.5412
 329/1250 [======>.......................] - ETA: 1:02 - loss: 1.1828 - acc: 0.5421
 332/1250 [======>.......................] - ETA: 1:01 - loss: 1.1814 - acc: 0.5424
 335/1250 [=======>......................] - ETA: 1:01 - loss: 1.1796 - acc: 0.5428
 338/1250 [=======>......................] - ETA: 1:00 - loss: 1.1779 - acc: 0.5436
 341/1250 [=======>......................] - ETA: 1:00 - loss: 1.1757 - acc: 0.5447
 344/1250 [=======>......................] - ETA: 59s - loss: 1.1743 - acc: 0.5458 
 347/1250 [=======>......................] - ETA: 58s - loss: 1.1723 - acc: 0.5464
 350/1250 [=======>......................] - ETA: 58s - loss: 1.1706 - acc: 0.5467
 353/1250 [=======>......................] - ETA: 57s - loss: 1.1698 - acc: 0.5466
 355/1250 [=======>......................] - ETA: 57s - loss: 1.1685 - acc: 0.5478
 358/1250 [=======>......................] - ETA: 57s - loss: 1.1668 - acc: 0.5480
 361/1250 [=======>......................] - ETA: 56s - loss: 1.1653 - acc: 0.5481
 364/1250 [=======>......................] - ETA: 56s - loss: 1.1638 - acc: 0.5489
 366/1250 [=======>......................] - ETA: 55s - loss: 1.1628 - acc: 0.5489
 369/1250 [=======>......................] - ETA: 55s - loss: 1.1610 - acc: 0.5495
 372/1250 [=======>......................] - ETA: 54s - loss: 1.1593 - acc: 0.5499
 375/1250 [========>.....................] - ETA: 54s - loss: 1.1580 - acc: 0.5495
 378/1250 [========>.....................] - ETA: 53s - loss: 1.1567 - acc: 0.5493
 381/1250 [========>.....................] - ETA: 53s - loss: 1.1551 - acc: 0.5491
 384/1250 [========>.....................] - ETA: 53s - loss: 1.1536 - acc: 0.5496
 387/1250 [========>.....................] - ETA: 52s - loss: 1.1522 - acc: 0.5493
 389/1250 [========>.....................] - ETA: 52s - loss: 1.1512 - acc: 0.5492
 392/1250 [========>.....................] - ETA: 51s - loss: 1.1495 - acc: 0.5497
 395/1250 [========>.....................] - ETA: 51s - loss: 1.1481 - acc: 0.5501
 398/1250 [========>.....................] - ETA: 51s - loss: 1.1461 - acc: 0.5511
 401/1250 [========>.....................] - ETA: 50s - loss: 1.1445 - acc: 0.5516
 404/1250 [========>.....................] - ETA: 50s - loss: 1.1427 - acc: 0.5517
 407/1250 [========>.....................] - ETA: 49s - loss: 1.1406 - acc: 0.5526
 410/1250 [========>.....................] - ETA: 49s - loss: 1.1388 - acc: 0.5533
 413/1250 [========>.....................] - ETA: 49s - loss: 1.1379 - acc: 0.5521
 415/1250 [========>.....................] - ETA: 48s - loss: 1.1369 - acc: 0.5521
 417/1250 [=========>....................] - ETA: 48s - loss: 1.1359 - acc: 0.5528
 420/1250 [=========>....................] - ETA: 48s - loss: 1.1344 - acc: 0.5528
 423/1250 [=========>....................] - ETA: 47s - loss: 1.1323 - acc: 0.5536
 426/1250 [=========>....................] - ETA: 47s - loss: 1.1307 - acc: 0.5540
 429/1250 [=========>....................] - ETA: 47s - loss: 1.1302 - acc: 0.5530
 432/1250 [=========>....................] - ETA: 46s - loss: 1.1289 - acc: 0.5526
 435/1250 [=========>....................] - ETA: 46s - loss: 1.1271 - acc: 0.5536
 438/1250 [=========>....................] - ETA: 45s - loss: 1.1256 - acc: 0.5539
 440/1250 [=========>....................] - ETA: 45s - loss: 1.1241 - acc: 0.5545
 443/1250 [=========>....................] - ETA: 45s - loss: 1.1227 - acc: 0.5545
 446/1250 [=========>....................] - ETA: 45s - loss: 1.1214 - acc: 0.5543
 449/1250 [=========>....................] - ETA: 44s - loss: 1.1199 - acc: 0.5547
 452/1250 [=========>....................] - ETA: 44s - loss: 1.1183 - acc: 0.5552
 455/1250 [=========>....................] - ETA: 44s - loss: 1.1166 - acc: 0.5558
 458/1250 [=========>....................] - ETA: 43s - loss: 1.1156 - acc: 0.5553
 461/1250 [==========>...................] - ETA: 43s - loss: 1.1141 - acc: 0.5559
 464/1250 [==========>...................] - ETA: 43s - loss: 1.1125 - acc: 0.5569
 467/1250 [==========>...................] - ETA: 42s - loss: 1.1112 - acc: 0.5568
 470/1250 [==========>...................] - ETA: 42s - loss: 1.1099 - acc: 0.5572
 473/1250 [==========>...................] - ETA: 42s - loss: 1.1084 - acc: 0.5574
 476/1250 [==========>...................] - ETA: 41s - loss: 1.1068 - acc: 0.5576
 479/1250 [==========>...................] - ETA: 41s - loss: 1.1055 - acc: 0.5570
 482/1250 [==========>...................] - ETA: 41s - loss: 1.1041 - acc: 0.5566
 485/1250 [==========>...................] - ETA: 40s - loss: 1.1027 - acc: 0.5569
 488/1250 [==========>...................] - ETA: 40s - loss: 1.1011 - acc: 0.5572
 491/1250 [==========>...................] - ETA: 40s - loss: 1.1000 - acc: 0.5569
 494/1250 [==========>...................] - ETA: 39s - loss: 1.0984 - acc: 0.5578
 497/1250 [==========>...................] - ETA: 39s - loss: 1.0971 - acc: 0.5577
 500/1250 [===========>..................] - ETA: 39s - loss: 1.0956 - acc: 0.5577
 503/1250 [===========>..................] - ETA: 39s - loss: 1.0939 - acc: 0.5582
 506/1250 [===========>..................] - ETA: 38s - loss: 1.0930 - acc: 0.5572
 509/1250 [===========>..................] - ETA: 38s - loss: 1.0912 - acc: 0.5583
 512/1250 [===========>..................] - ETA: 38s - loss: 1.0901 - acc: 0.5579
 515/1250 [===========>..................] - ETA: 37s - loss: 1.0885 - acc: 0.5580
 518/1250 [===========>..................] - ETA: 37s - loss: 1.0874 - acc: 0.5580
 521/1250 [===========>..................] - ETA: 37s - loss: 1.0864 - acc: 0.5576
 524/1250 [===========>..................] - ETA: 37s - loss: 1.0849 - acc: 0.5581
 527/1250 [===========>..................] - ETA: 36s - loss: 1.0834 - acc: 0.5582
 530/1250 [===========>..................] - ETA: 36s - loss: 1.0824 - acc: 0.5581
 533/1250 [===========>..................] - ETA: 36s - loss: 1.0811 - acc: 0.5583
 536/1250 [===========>..................] - ETA: 36s - loss: 1.0802 - acc: 0.5578
 539/1250 [===========>..................] - ETA: 35s - loss: 1.0788 - acc: 0.5580
 542/1250 [============>.................] - ETA: 35s - loss: 1.0774 - acc: 0.5581
 545/1250 [============>.................] - ETA: 35s - loss: 1.0763 - acc: 0.5576
 548/1250 [============>.................] - ETA: 34s - loss: 1.0748 - acc: 0.5581
 551/1250 [============>.................] - ETA: 34s - loss: 1.0737 - acc: 0.5577
 554/1250 [============>.................] - ETA: 34s - loss: 1.0724 - acc: 0.5575
 557/1250 [============>.................] - ETA: 34s - loss: 1.0711 - acc: 0.5580
 560/1250 [============>.................] - ETA: 33s - loss: 1.0700 - acc: 0.5577
 563/1250 [============>.................] - ETA: 33s - loss: 1.0688 - acc: 0.5581
 566/1250 [============>.................] - ETA: 33s - loss: 1.0678 - acc: 0.5575
 569/1250 [============>.................] - ETA: 33s - loss: 1.0665 - acc: 0.5577
 571/1250 [============>.................] - ETA: 33s - loss: 1.0656 - acc: 0.5577
 574/1250 [============>.................] - ETA: 32s - loss: 1.0645 - acc: 0.5574
 577/1250 [============>.................] - ETA: 32s - loss: 1.0633 - acc: 0.5576